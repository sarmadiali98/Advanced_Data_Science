{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xmZMK878aSRn",
        "GHeLRQZEaX9v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# part 1, 2, 3"
      ],
      "metadata": {
        "id": "xmZMK878aSRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBGH0L2jTeUd",
        "outputId": "db48c8bf-b641-47b1-e303-684093cf5578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cardetection.zip to /content\n",
            " 81% 78.0M/96.6M [00:00<00:00, 216MB/s]\n",
            "100% 96.6M/96.6M [00:00<00:00, 207MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Step 4: Download the dataset\n",
        "!kaggle datasets download -d pkdarabi/cardetection\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cardetection.zip"
      ],
      "metadata": {
        "id": "pKG_k5ywXx06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The traffic sign image dataset comprises 4969 samples meticulously divided into three parts—Train, Valid, and Test. Featuring distinct classes such as Green Light, Red Light, Speed Limits ranging from 10 to 120, and Stop signs, the dataset presents a valuable resource for machine learning applications. With applications in autonomous vehicle navigation, the model derived from this dataset can empower self-driving cars to accurately recognize and respond to various traffic signs, ensuring adherence to traffic regulations. Additionally, the dataset finds utility in driver assistance systems, enhancing road safety by monitoring and alerting drivers who may violate speed limits or ignore red lights. Furthermore, it can serve as a foundation for road safety training programs, enabling simulations for new drivers to better understand and respond to different traffic scenarios. Beyond individual vehicles, the dataset can contribute to smart city initiatives, allowing authorities to employ the model in connected infrastructure for real-time monitoring of traffic compliance."
      ],
      "metadata": {
        "id": "44sUHr43kXp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define the directories\n",
        "directories = ['/content/train', '/content/valid', '/content/test']\n",
        "\n",
        "# Define the model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=2, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Prepare the image data generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for directory in directories[:-1]:  # Exclude the test directory\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    train_data = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    # Create a new model for each fold\n",
        "    model = create_model()\n",
        "\n",
        "    # Fit data to model\n",
        "    model.fit(train_data, epochs=10, verbose=1)\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "# Validation on the 'valid' set\n",
        "valid_directory = '/content/valid'\n",
        "valid_data = datagen.flow_from_directory(\n",
        "    valid_directory,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create a new model for validation\n",
        "model = create_model()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.evaluate(valid_data, verbose=1)\n",
        "\n",
        "# Testing on the 'test' set\n",
        "test_directory = '/content/test'\n",
        "test_data = datagen.flow_from_directory(\n",
        "    test_directory,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create a new model for testing\n",
        "model = create_model()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(test_data, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W6JDpNUX4Zy",
        "outputId": "fcb054d8-4a91-42a2-dcf1-9b37781c60b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for fold 1 ...\n",
            "Found 3530 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "111/111 [==============================] - 10s 87ms/step - loss: 0.0053 - accuracy: 0.9997\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 9s 84ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 10s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "111/111 [==============================] - 10s 86ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "111/111 [==============================] - 10s 86ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "111/111 [==============================] - 10s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "111/111 [==============================] - 10s 85ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "111/111 [==============================] - 10s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "111/111 [==============================] - 9s 84ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "111/111 [==============================] - 10s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Training for fold 2 ...\n",
            "Found 801 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "26/26 [==============================] - 3s 99ms/step - loss: 0.0259 - accuracy: 0.9938\n",
            "Epoch 2/10\n",
            "26/26 [==============================] - 2s 77ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "26/26 [==============================] - 2s 78ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "26/26 [==============================] - 2s 76ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "26/26 [==============================] - 3s 98ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "26/26 [==============================] - 2s 79ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "26/26 [==============================] - 2s 77ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "26/26 [==============================] - 2s 75ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "26/26 [==============================] - 2s 76ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "26/26 [==============================] - 2s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Found 801 images belonging to 2 classes.\n",
            "26/26 [==============================] - 1s 35ms/step - loss: 0.6367 - accuracy: 0.8789\n",
            "Found 638 images belonging to 2 classes.\n",
            "20/20 [==============================] - 1s 36ms/step - loss: 0.7797 - accuracy: 0.0157\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7796540856361389, 0.015673981979489326]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is for training, validating, and testing a Convolutional Neural Network (CNN) model using Keras and Scikit-learn libraries. The CNN model is defined with a sequential architecture consisting of a convolutional layer, a max pooling layer, a flattening layer, and two dense layers. The model is trained using images from the ‘train’ and ‘valid’ directories, and then evaluated on the ‘valid’ and ‘test’ directories. The images are preprocessed using Keras’s ImageDataGenerator to rescale the pixel values. The model is trained and evaluated separately for each directory in a form of K-fold Cross Validation, where K is the number of directories. The model’s performance is evaluated based on its accuracy on the validation and test sets. The code is designed to print the training progress and the evaluation results."
      ],
      "metadata": {
        "id": "EBsdPcLBkk8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define the directories\n",
        "directories = ['/content/train', '/content/valid', '/content/test']\n",
        "\n",
        "# Define the model\n",
        "def create_model(kernel_size, conv_stride, pool_size, pool_stride):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size, strides=conv_stride, activation='relu', input_shape=(64, 64, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size, strides=pool_stride))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=2, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Prepare the image data generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Define configurations\n",
        "configs = [((3, 3), (1, 1), (2, 2), (2, 2)), ((5, 5), (2, 2), (3, 3), (2, 2))]\n",
        "\n",
        "# Variables to store the best configuration and its accuracy\n",
        "best_acc = 0\n",
        "best_config = None\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for config in configs:\n",
        "    kernel_size, conv_stride, pool_size, pool_stride = config\n",
        "    print(f'Testing configuration: {config}')\n",
        "\n",
        "    for directory in directories[:-1]:  # Exclude the test directory\n",
        "        print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "        train_data = datagen.flow_from_directory(\n",
        "            directory,\n",
        "            target_size=(64, 64),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "        # Create a new model for each fold\n",
        "        model = create_model(kernel_size, conv_stride, pool_size, pool_stride)\n",
        "\n",
        "        # Fit data to model\n",
        "        history = model.fit(train_data, epochs=3, verbose=1)\n",
        "\n",
        "        # Check if this configuration is the best\n",
        "        if max(history.history['accuracy']) > best_acc:\n",
        "            best_acc = max(history.history['accuracy'])\n",
        "            best_config = config\n",
        "            # Save the best model\n",
        "            model.save('best_model.h5')\n",
        "\n",
        "        fold_no = fold_no + 1\n",
        "\n",
        "print(f'Best configuration: {best_config}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMB1p5rTz6_6",
        "outputId": "1b912ac2-29be-4843-dada-dbe89de52209"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing configuration: ((3, 3), (1, 1), (2, 2), (2, 2))\n",
            "Training for fold 1 ...\n",
            "Found 3530 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "111/111 [==============================] - 10s 88ms/step - loss: 0.0063 - accuracy: 0.9949\n",
            "Epoch 2/3\n",
            "111/111 [==============================] - 10s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "111/111 [==============================] - 10s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Training for fold 2 ...\n",
            "Found 801 images belonging to 2 classes.\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/26 [==============================] - 3s 76ms/step - loss: 0.0272 - accuracy: 0.9850\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 2s 76ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Testing configuration: ((5, 5), (2, 2), (3, 3), (2, 2))\n",
            "Training for fold 3 ...\n",
            "Found 3530 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "111/111 [==============================] - 5s 41ms/step - loss: 0.0064 - accuracy: 0.9975\n",
            "Epoch 2/3\n",
            "111/111 [==============================] - 5s 43ms/step - loss: 3.5353e-07 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "111/111 [==============================] - 5s 46ms/step - loss: 2.7842e-07 - accuracy: 1.0000\n",
            "Training for fold 4 ...\n",
            "Found 801 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 2s 55ms/step - loss: 0.0385 - accuracy: 0.9600\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 1s 40ms/step - loss: 7.5451e-07 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 1s 41ms/step - loss: 4.0272e-07 - accuracy: 1.0000\n",
            "Best configuration: ((3, 3), (1, 1), (2, 2), (2, 2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is designed to test different configurations of a Convolutional Neural Network (CNN) model for image classification. It uses Keras and Scikit-learn libraries to create the CNN model with varying kernel size and stride for convolutional layers, and varying pooling size and stride for pooling layers. The model is trained and evaluated on images from different directories using K-fold Cross Validation. The code prints the configuration being tested, and at the end, it prints the best configuration based on accuracy and saves the corresponding model. The configurations are defined in a list, and the best configuration is determined by comparing the accuracy of the current configuration with the highest accuracy found so far."
      ],
      "metadata": {
        "id": "FI6GP1yn5Mic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 4"
      ],
      "metadata": {
        "id": "GHeLRQZEaX9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define the directories\n",
        "directories = ['/content/train', '/content/valid', '/content/test']\n",
        "\n",
        "# Define the model\n",
        "def create_model(kernel_size, conv_stride, pool_size, pool_stride):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size, strides=conv_stride, activation='relu', input_shape=(64, 64, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=pool_size, strides=pool_stride))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=128, activation='relu'))\n",
        "    model.add(Dense(units=2, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Prepare the image data generator with data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define configurations\n",
        "configs = [((3, 3), (1, 1), (2, 2), (2, 2)), ((5, 5), (2, 2), (3, 3), (2, 2))]\n",
        "\n",
        "# Variables to store the best configuration and its accuracy\n",
        "best_acc = 0\n",
        "best_config = None\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for config in configs:\n",
        "    kernel_size, conv_stride, pool_size, pool_stride = config\n",
        "    print(f'Testing configuration: {config}')\n",
        "\n",
        "    for directory in directories[:-1]:  # Exclude the test directory\n",
        "        print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "        train_data = datagen.flow_from_directory(\n",
        "            directory,\n",
        "            target_size=(64, 64),\n",
        "            batch_size=32,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "        # Create a new model for each fold\n",
        "        model = create_model(kernel_size, conv_stride, pool_size, pool_stride)\n",
        "\n",
        "        # Fit data to model\n",
        "        history = model.fit(train_data, epochs=3, verbose=1)\n",
        "\n",
        "        # Check if this configuration is the best\n",
        "        if max(history.history['accuracy']) > best_acc:\n",
        "            best_acc = max(history.history['accuracy'])\n",
        "            best_config = config\n",
        "            # Save the best model\n",
        "            model.save('best_model.h5')\n",
        "\n",
        "        fold_no = fold_no + 1\n",
        "\n",
        "print(f'Best configuration: {best_config}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_nxIt7f4-g_",
        "outputId": "8d670613-e49b-47d7-ba14-309a0f9c4500"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing configuration: ((3, 3), (1, 1), (2, 2), (2, 2))\n",
            "Training for fold 1 ...\n",
            "Found 3530 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "111/111 [==============================] - 13s 114ms/step - loss: 0.0079 - accuracy: 0.9909\n",
            "Epoch 2/3\n",
            "111/111 [==============================] - 12s 111ms/step - loss: 5.1331e-09 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "111/111 [==============================] - 13s 114ms/step - loss: 2.8029e-09 - accuracy: 1.0000\n",
            "Training for fold 2 ...\n",
            "Found 801 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 4s 132ms/step - loss: 0.0216 - accuracy: 1.0000\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 3s 100ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 3s 98ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Testing configuration: ((5, 5), (2, 2), (3, 3), (2, 2))\n",
            "Training for fold 3 ...\n",
            "Found 3530 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "111/111 [==============================] - 8s 68ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 2/3\n",
            "111/111 [==============================] - 8s 70ms/step - loss: 9.1849e-08 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "111/111 [==============================] - 8s 74ms/step - loss: 7.4696e-08 - accuracy: 1.0000\n",
            "Training for fold 4 ...\n",
            "Found 801 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "26/26 [==============================] - 2s 69ms/step - loss: 0.0329 - accuracy: 0.9638\n",
            "Epoch 2/3\n",
            "26/26 [==============================] - 2s 74ms/step - loss: 3.4825e-08 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "26/26 [==============================] - 2s 67ms/step - loss: 9.2272e-09 - accuracy: 1.0000\n",
            "Best configuration: ((3, 3), (1, 1), (2, 2), (2, 2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is designed to train a Convolutional Neural Network (CNN) model for image classification with different configurations and data augmentation. It uses the Keras library to define the CNN model and the ImageDataGenerator class for real-time data augmentation. The code tests different configurations of kernel size, stride for convolutional layers, and pooling size, stride for pooling layers. For each configuration, the model is trained on images from different directories and the accuracy is evaluated. The configuration yielding the highest accuracy is considered the best, and the corresponding model is saved. The code prints the configuration being tested and, at the end, the best configuration. Data augmentation techniques used include rotation, width and height shifts, shear transformation, zooming, and horizontal flipping."
      ],
      "metadata": {
        "id": "tht2xFiFbmZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 5"
      ],
      "metadata": {
        "id": "Pv5gP-sab0iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG19, ResNet50\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load pre-trained models\n",
        "vgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "resnet50 = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "\n",
        "# Freeze the layers\n",
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in resnet50.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create new models for transfer learning\n",
        "model_vgg19 = Sequential([\n",
        "    vgg19,\n",
        "    Flatten(),\n",
        "    Dense(units=128, activation='relu'),\n",
        "    Dense(units=2, activation='softmax')\n",
        "])\n",
        "\n",
        "model_resnet50 = Sequential([\n",
        "    resnet50,\n",
        "    Flatten(),\n",
        "    Dense(units=128, activation='relu'),\n",
        "    Dense(units=2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the models\n",
        "model_vgg19.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model_resnet50.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the image data generator\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Define the directory\n",
        "directory = '/content/train'\n",
        "\n",
        "# Prepare the training data\n",
        "train_data = datagen.flow_from_directory(\n",
        "    directory,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Train the models\n",
        "model_vgg19.fit(train_data, epochs=3, verbose=1)\n",
        "model_resnet50.fit(train_data, epochs=3, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG4Gyu8ab5wZ",
        "outputId": "41ef51fd-c44e-40b9-ffd0-671ddc24e82a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3530 images belonging to 2 classes.\n",
            "Epoch 1/3\n",
            "111/111 [==============================] - 138s 1s/step - loss: 0.0083 - accuracy: 0.9932\n",
            "Epoch 2/3\n",
            "111/111 [==============================] - 137s 1s/step - loss: 1.8911e-09 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "111/111 [==============================] - 136s 1s/step - loss: 1.8911e-09 - accuracy: 1.0000\n",
            "Epoch 1/3\n",
            "111/111 [==============================] - 36s 302ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 2/3\n",
            "111/111 [==============================] - 33s 300ms/step - loss: 2.5328e-08 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "111/111 [==============================] - 34s 300ms/step - loss: 3.2420e-09 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c7143c84b50>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs transfer learning using the VGG19 and ResNet50 models from Keras applications for image classification. The pre-trained models are loaded with weights trained on ImageNet, and their layers are frozen to preserve the learned features. New models are created by adding a flatten layer and two dense layers to the pre-trained models. The models are compiled and trained on training data prepared using Keras’s ImageDataGenerator. The code is designed to train the models on images from a specified directory and print the training progress."
      ],
      "metadata": {
        "id": "8PAg3_dFdBIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 7"
      ],
      "metadata": {
        "id": "b2UBe7cbdD8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the receptive field in convolutional layers, often referred to as the kernel size, can significantly impact the performance of a neural network.\n",
        "\n",
        "When the kernel size is increased, the model becomes capable of capturing more global information in the input. Larger kernels can detect larger and more complex patterns in the image. However, this comes at the cost of losing some local details and requires more computational resources due to the increase in the number of parameters.\n",
        "\n",
        "On the other hand, decreasing the kernel size allows the model to capture more local and detailed information in the input. Smaller kernels can detect smaller and simpler patterns in the image. This can be beneficial for tasks that require fine-grained understanding of the image. However, it might fail to capture larger patterns and the overall structure of the image.\n",
        "\n",
        "Therefore, the choice of kernel size is a trade-off between capturing global structural information and local detailed information, and should be chosen based on the specific task and dataset. It’s also common to use a combination of different kernel sizes in the same model to capture features at various scales.\n",
        "\n",
        "It is important to note that these are general observations and the actual impact may vary depending on the specific task, dataset, and overall architecture of the neural network. It’s always a good idea to experiment with different kernel sizes and observe their impact on the model’s performance."
      ],
      "metadata": {
        "id": "tUp0n5yfdHre"
      }
    }
  ]
}