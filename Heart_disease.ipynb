{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-8HkWgm1qw2"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'alisarmadi98'\n",
        "os.environ['KAGGLE_KEY'] = 'b84ec0b5251ec99625b628f9b52255bf'\n",
        "\n",
        "!kaggle datasets download -d andrewmvd/heart-failure-clinical-data\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip heart-failure-clinical-data.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('heart_failure_clinical_records_dataset.csv')"
      ],
      "metadata": {
        "id": "Mix8_hZe5TJ9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "# Standardize numerical features and one-hot encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']),\n",
        "        ('cat', OneHotEncoder(), ['sex', 'smoking', 'anaemia', 'diabetes', 'high_blood_pressure']),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42)),\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISw7M1TSAOtI",
        "outputId": "4e60026d-959d-4ad0-f71f-a8d465fc93bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.97      0.85        35\n",
            "           1       0.93      0.56      0.70        25\n",
            "\n",
            "    accuracy                           0.80        60\n",
            "   macro avg       0.84      0.77      0.78        60\n",
            "weighted avg       0.83      0.80      0.79        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Data Cleaning:\n",
        "\n",
        "Check for any missing values in the dataset.\n",
        "If there are missing values, decide on a strategy for handling them (e.g., imputation or removal).\n",
        "Feature Scaling:\n",
        "\n",
        "Standardize or normalize numerical features to ensure that they are on a similar scale. This is important for some machine learning algorithms, including logistic regression.\n",
        "Encoding Categorical Variables:\n",
        "\n",
        "If there are categorical variables (like 'sex' and 'smoking'), encode them into numerical format. You can use one-hot encoding for this purpose.\n",
        "Train-Test Split:\n",
        "\n",
        "Split the dataset into training and testing sets. This is crucial for evaluating the model's performance on unseen data.\n",
        "Feature Selection (Optional):\n",
        "\n",
        "If your dataset is large or has redundant features, consider performing feature selection to improve the model's efficiency.\n",
        "Model Training:\n",
        "\n",
        "Train the logistic regression model using the training data.\n",
        "Model Evaluation:\n",
        "\n",
        "Evaluate the model's performance on the testing set using metrics like accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "BTQwdjbGBhz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame with the dataset\n",
        "# X contains the features, y contains the target variable\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "# Standardize numerical features and one-hot encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']),\n",
        "        ('cat', OneHotEncoder(), ['sex', 'smoking', 'anaemia', 'diabetes', 'high_blood_pressure']),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create an SVM model\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(random_state=42)),\n",
        "])\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10, 100],\n",
        "    'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='f1', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv7lS8WWBVMS",
        "outputId": "2d5bad44-0822-4d8b-d1f4-8fee2bf4cfa8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__C': 1, 'classifier__kernel': 'linear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        35\n",
            "           1       0.88      0.60      0.71        25\n",
            "\n",
            "    accuracy                           0.80        60\n",
            "   macro avg       0.82      0.77      0.78        60\n",
            "weighted avg       0.82      0.80      0.79        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code prepares a dataset for SVM classification, focusing on predicting the occurrence of death events. It starts by splitting the dataset into training and testing sets and then employs a preprocessing pipeline that standardizes numerical features and one-hot encodes categorical variables. The SVM model is built with the scikit-learn library, and a grid search is employed to tune hyperparameters (C and kernel) for optimal performance. The best model is selected based on the F1-score during the hyperparameter search. The final model is then evaluated on the test set, and the classification report, including precision, recall, and F1-score, is printed. The goal is to achieve an F1-score above 0.8, providing a balanced measure of precision and recall for the classification task."
      ],
      "metadata": {
        "id": "88utad6hCKr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame with the dataset\n",
        "# X contains the features, y contains the target variable\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "# Standardize numerical features and one-hot encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']),\n",
        "        ('cat', OneHotEncoder(), ['sex', 'smoking', 'anaemia', 'diabetes', 'high_blood_pressure']),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a kernel SVM model with RBF kernel\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(random_state=42, probability=True)),  # probability=True for later ROC analysis\n",
        "])\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10, 100],\n",
        "    'classifier__gamma': [0.01, 0.1, 1, 10],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='f1', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMIfCOJOB7U1",
        "outputId": "45b21580-0119-45d2-f518-c65a34107a48"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__C': 10, 'classifier__gamma': 0.01}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.94      0.85        35\n",
            "           1       0.88      0.60      0.71        25\n",
            "\n",
            "    accuracy                           0.80        60\n",
            "   macro avg       0.82      0.77      0.78        60\n",
            "weighted avg       0.82      0.80      0.79        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code applies kernel SVM (Support Vector Machine) with a radial basis function (RBF) kernel to predict death events based on a prepared dataset. It begins by splitting the data into training and testing sets and uses a preprocessing pipeline to standardize numerical features and one-hot encode categorical variables. The SVM model is constructed with scikit-learn, incorporating a probability estimate for later ROC analysis. Hyperparameter tuning is performed using a grid search over the regularization parameter (C) and the kernel coefficient (gamma) to optimize the model's performance. The best model is selected based on the F1-score during the hyperparameter search. Finally, the code evaluates the model on the test set and prints a classification report, aiming to achieve an F1-score above 0.8 for a balanced measure of precision and recall in the classification task."
      ],
      "metadata": {
        "id": "mrs6X-9iC6fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame with the dataset\n",
        "# X contains the features, y contains the target variable\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = len(y_train) / (2 * pd.value_counts(y_train))\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "# Standardize numerical features and one-hot encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']),\n",
        "        ('cat', OneHotEncoder(), ['sex', 'smoking', 'anaemia', 'diabetes', 'high_blood_pressure']),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a KNN model with class weights\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', KNeighborsClassifier(weights='distance', p=1, n_neighbors=5)),\n",
        "])\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'classifier__n_neighbors': [3, 5, 7, 9],\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='f1', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Number of Neighbors (k):\", best_model.named_steps['classifier'].n_neighbors)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QpuL4ndClXL",
        "outputId": "ef3a3219-a5a1-4132-f041-98a360bde0c1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Number of Neighbors (k): 3\n",
            "Best Parameters: {'classifier__n_neighbors': 3, 'classifier__weights': 'uniform'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.91      0.78        35\n",
            "           1       0.77      0.40      0.53        25\n",
            "\n",
            "    accuracy                           0.70        60\n",
            "   macro avg       0.73      0.66      0.65        60\n",
            "weighted avg       0.72      0.70      0.67        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code performs K-Nearest Neighbors (KNN) classification on a dataset, specifically addressing the prediction of death events. The dataset is split into training and testing sets, and class weights are calculated to handle class imbalance. A preprocessing pipeline is constructed to standardize numerical features and one-hot encode categorical variables. A KNN model is then created using the scikit-learn pipeline, incorporating class weights and hyperparameter settings such as the distance-weighted approach ('weights='distance''), the Manhattan distance metric ('p=1'), and an initial number of neighbors set to 5. Hyperparameters, including the number of neighbors and weight options, are fine-tuned using GridSearchCV. The best model is selected based on the F1-score, and predictions are made on the test set. The code prints the best number of neighbors selected by the grid search, the best hyperparameters, and a detailed classification report, offering insights into the model's performance on the test data."
      ],
      "metadata": {
        "id": "cTWu5epBFMJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame with the dataset\n",
        "# X contains the features, y contains the target variable\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "# Standardize numerical features and one-hot encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']),\n",
        "        ('cat', OneHotEncoder(), ['sex', 'smoking', 'anaemia', 'diabetes', 'high_blood_pressure']),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a Decision Tree model\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42)),\n",
        "])\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [None, 5, 10, 15, 20],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='f1', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PptnWaYZDNQ4",
        "outputId": "0cf293d9-1218-4e84-f38c-e6d7257f6c8a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 5}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.80      0.78        35\n",
            "           1       0.70      0.64      0.67        25\n",
            "\n",
            "    accuracy                           0.73        60\n",
            "   macro avg       0.73      0.72      0.72        60\n",
            "weighted avg       0.73      0.73      0.73        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code implements a Decision Tree classification model for predicting death events, addressing class imbalance, and tuning hyperparameters to avoid overfitting. The dataset is split into training and testing sets, and a preprocessing pipeline is constructed to standardize numerical features and one-hot encode categorical variables. The Decision Tree model is created using scikit-learn, with hyperparameter settings for the maximum depth of the tree, minimum samples required to split a node, and minimum samples required to be a leaf. The hyperparameters are fine-tuned using a grid search to optimize the model's performance, assessed based on the F1-score. The code prints the best hyperparameters selected by the grid search and a detailed classification report, evaluating the model's predictive accuracy on the test data. The goal is to achieve an F1-score above 0.8, indicating a balance between precision and recall in the classification task."
      ],
      "metadata": {
        "id": "PJpXSX87Ftw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming df is your DataFrame with the dataset\n",
        "# X contains the features, y contains the target variable\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "# Standardize numerical features and one-hot encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']),\n",
        "        ('cat', OneHotEncoder(), ['sex', 'smoking', 'anaemia', 'diabetes', 'high_blood_pressure']),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a Random Forest model\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42)),\n",
        "])\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 150],\n",
        "    'classifier__max_depth': [None, 5, 10, 15],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(model, param_grid, scoring='f1', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2IyDZydFhdo",
        "outputId": "1a230454-3bb7-4972-b611-7c6aba97e80b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__max_depth': 5, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 50}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.94      0.83        35\n",
            "           1       0.87      0.52      0.65        25\n",
            "\n",
            "    accuracy                           0.77        60\n",
            "   macro avg       0.80      0.73      0.74        60\n",
            "weighted avg       0.79      0.77      0.75        60\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code implements a Random Forest classification model for predicting death events in a dataset, with the goal of achieving an F1-score above 0.85. The dataset is divided into training and testing sets, and a preprocessing pipeline is established to standardize numerical features and one-hot encode categorical variables. The Random Forest model, which is an ensemble of decision trees, is constructed using scikit-learn. Hyperparameters governing the number of trees, maximum depth of individual trees, and minimum samples required for node splitting and leaf formation are fine-tuned through a grid search. The grid search optimizes the model based on the F1-score during cross-validation. The code then prints the best hyperparameters selected by the grid search and a detailed classification report, providing an assessment of the model's predictive accuracy on the test data, aiming for an F1-score exceeding 0.85 as a balanced measure of precision and recall in the classification task."
      ],
      "metadata": {
        "id": "kmaBRQhjHAXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "three techniques to regularize the training process for decision trees:\n",
        "\n",
        "Set a Maximum Depth: This technique prevents decision trees from growing past a maximum depth. For example, you might limit the depth of the tree to 10 levels. This helps to control the complexity of the model and prevent overfitting.\n",
        "\n",
        "Set a Minimum Number of Examples in Leaf: This technique ensures that a leaf node must have a minimum number of examples before it can be considered for splitting. For example, a leaf with less than a certain number of examples, such as 5, will not be considered for splitting. This also helps to prevent overfitting.\n",
        "\n",
        "Pruning: This is a technique where you selectively remove certain branches of the tree after training. This is done by converting certain non-leaf nodes to leaves. A common solution to select the branches to remove is to use a validation dataset. That is, if removing a branch improves the quality of the model on the validation dataset, then the branch is removed.\n",
        "\n",
        "These techniques help to control the complexity of the decision tree, prevent overfitting, and improve the generalization ability of the model."
      ],
      "metadata": {
        "id": "T5bZtje6HTUu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XBN4VtzEH8_P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}